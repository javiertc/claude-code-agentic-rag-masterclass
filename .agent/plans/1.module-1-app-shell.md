# Module 1: App Shell + Observability

**Complexity:** :warning: **Medium** -- Multiple subsystems (auth, chat, streaming, observability) but well-understood patterns. SSE streaming integration may need iteration.

## Prerequisites
- Supabase project created (need: project URL, anon key, service role key, JWT secret)
- OpenAI API key with access to Responses API
- LangSmith account + API key
- Node.js 18+, Python 3.9+

---

## Task 1: Project Scaffolding

Set up monorepo with `frontend/` and `backend/` directories.

**Frontend (`frontend/`):**
- `npm create vite@latest` with `react-ts` template
- Tailwind CSS v4 via `@tailwindcss/vite` plugin
- shadcn/ui (New York style, Neutral, CSS variables)
- Path aliases: `@/*` -> `./src/*`
- Dependencies: `react-router-dom@^7.13.0`, `@supabase/supabase-js@^2.95.3`, `@microsoft/fetch-event-source@^2.0.1`

**Backend (`backend/`):**
- Python venv
- `requirements.txt`: `fastapi[standard]==0.128.2`, `uvicorn[standard]==0.39.0`, `openai==2.17.0`, `pydantic==2.12.5`, `pydantic-settings==2.11.0`, `pyjwt==2.11.0`, `python-dotenv==1.1.0`, `sse-starlette==3.2.0`, `supabase==2.27.3`, `langsmith==0.4.37`, `python-multipart==0.0.20`, `psycopg2-binary==2.9.11`
- `main.py`: FastAPI app with CORS middleware (allow `localhost:5173`)
- `config.py`: Pydantic `BaseSettings` loading from `.env`

**Env files:** `.env.example` for both frontend and backend

**Validate:** Both dev servers start, `/health` returns OK

---

## Task 2: Supabase Schema + RLS

Run via Supabase SQL Editor:

```sql
-- threads table: conversation metadata per user
create table public.threads (
  id uuid primary key default gen_random_uuid(),
  user_id uuid not null references auth.users(id) on delete cascade,
  title text not null default 'New Chat',
  last_response_id text,        -- OpenAI response ID for threading
  vector_store_id text,         -- OpenAI vector store ID for file_search
  created_at timestamptz not null default now(),
  updated_at timestamptz not null default now()
);

create index idx_threads_user_id on public.threads(user_id);
create index idx_threads_updated_at on public.threads(updated_at desc);
alter table public.threads enable row level security;

-- RLS policies (select, insert, update, delete) using (select auth.uid()) = user_id
-- updated_at trigger function
```

**Key decision:** No messages table in Module 1. OpenAI manages conversation history via `previous_response_id` + `store=True`. We only store thread metadata.

**Validate:** Query returns empty result, RLS blocks cross-user access

---

## Task 3: Authentication

### 3A: Backend (`backend/auth.py`)
- `HTTPBearer` security scheme
- `get_current_user` dependency: decode JWT with `pyjwt`, HS256, audience `"authenticated"`, extract `user_id` from `sub` claim

### 3B: Frontend
- `lib/supabase.ts` -- Supabase client singleton
- `contexts/AuthContext.tsx` -- Session state, `onAuthStateChange` listener, signUp/signIn/signOut methods
- `components/ProtectedRoute.tsx` -- Redirects to `/login` if no session
- `pages/LoginPage.tsx` -- Email/password form with Sign In / Sign Up tabs (shadcn: card, button, input, label, tabs)
- `App.tsx` -- React Router: `/login` public, `/` protected -> ChatPage

**shadcn components to install:** card, button, input, label, tabs, toast/sonner

**Validate:** Signup works, login works, unauthenticated redirects to /login, backend rejects requests without valid JWT

---

## Task 4: Thread CRUD API

**Files:** `backend/database.py`, `backend/routers/threads.py`, `backend/models/threads.py`

- `database.py`: Supabase client using service role key
- Endpoints (all require auth):
  - `GET /api/threads/` -- list user's threads (ordered by updated_at desc)
  - `POST /api/threads/` -- create thread
  - `GET /api/threads/{id}` -- get single thread
  - `PATCH /api/threads/{id}` -- update title
  - `DELETE /api/threads/{id}` -- delete thread

**Note:** Service role key bypasses RLS; we filter by `user_id` in every query for security.

**Validate:** Full CRUD works via curl with auth header, cross-user access denied

---

## Task 5: Chat UI

**Frontend components:**
- `components/chat/ChatLayout.tsx` -- Flex: sidebar (280px) + chat area
- `components/chat/ThreadSidebar.tsx` -- "New Chat" button + thread list
- `components/chat/ThreadItem.tsx` -- Single thread row (title, active highlight, delete)
- `components/chat/ChatArea.tsx` -- Header + MessageList + ChatInput
- `components/chat/MessageList.tsx` -- Scrollable message container
- `components/chat/MessageBubble.tsx` -- User/assistant message styling
- `components/chat/ChatInput.tsx` -- Textarea + send button

**Helpers:**
- `lib/api.ts` -- `apiFetch()` with auth header injection
- `hooks/useThreads.ts` -- Thread CRUD hooks
- `types/index.ts` -- Thread, Message interfaces

**Additional shadcn:** scroll-area, separator, avatar, textarea, tooltip

**State:** Messages stored in React state only (cleared on thread switch). No message persistence in Module 1.

**Validate:** Can create/select/delete threads, type messages (sending wired in Task 7)

---

## Task 6: OpenAI Responses API Integration (Backend)

**Files:** `backend/services/openai_service.py`, `backend/routers/chat.py`, `backend/models/chat.py`

**`openai_service.py`:**
- `wrap_openai(OpenAI(...))` for LangSmith auto-tracing
- `initiate_chat(message, previous_response_id, vector_store_id)` -- calls `client.responses.create()` with `stream=True`, `store=True`, file_search tool if vector_store_id exists

**`routers/chat.py`:**
- `POST /api/chat/` -- accepts `{message, thread_id}`
- Fetches thread to get `last_response_id` and `vector_store_id`
- Iterates OpenAI stream events, yields SSE:
  - `response.created` -> `event: response_id`
  - `response.output_text.delta` -> `event: text_delta`
  - `response.completed` -> updates thread's `last_response_id`, yields `event: done`
- Returns `EventSourceResponse` (from `sse-starlette`)

**Model:** Configurable via env var, default `gpt-4.1-mini`

**Validate:** curl to `/api/chat/` returns SSE stream, follow-up messages show model remembers context

---

## Task 7: SSE Streaming (Frontend)

**Files:** `lib/api.ts` (add `streamChat`), `hooks/useChat.ts`

**`streamChat()`:** Uses `@microsoft/fetch-event-source` to consume SSE from `/api/chat/`. Callbacks: `onTextDelta`, `onResponseId`, `onDone`, `onError`.

**`useChat` hook:**
- `messages` state (array of Message)
- `sendMessage(content)` -- appends user message, creates empty assistant message, streams deltas into it
- `isStreaming` flag -- disables input during streaming
- `clearMessages()` -- called on thread switch
- Auto-scroll to bottom

**Validate:** Full chat works end-to-end: type message -> streams response token-by-token -> follow-ups remember context

---

## Task 8: File Upload for file_search

**Backend:**
- `openai_service.py`: `get_or_create_vector_store(thread_id)` + `upload_file_to_vector_store(vector_store_id, file, filename)`
- `routers/files.py`: `POST /api/files/upload/{thread_id}` -- multipart file upload, creates/reuses vector store per thread

**Frontend:**
- `components/chat/FileUploadButton.tsx` -- Paperclip icon, native file picker (`.pdf,.txt,.md,.docx,.html`)
- Integrate into `ChatInput.tsx` -- upload on file select, show filename chip
- Upload happens immediately (before message send) so file_search is ready

**Validate:** Upload file -> ask about its content -> model answers using file_search -> verify in LangSmith trace

---

## Task 9: LangSmith Tracing

Mostly done via `wrap_openai` in Task 6. Additional:
- Verify env vars are set: `LANGSMITH_TRACING=true`, `LANGSMITH_API_KEY`, `LANGSMITH_PROJECT`
- Add `@traceable` to key orchestration functions if needed
- Verify traces appear in LangSmith dashboard with: model, input, output, token usage, latency, tool calls

**Validate:** Every chat message creates a trace in LangSmith, file_search tool calls visible

---

## Task 10: E2E Validation + Progress Update

**Test scenarios:**
1. New user signup -> empty threads -> create thread -> chat -> model responds -> context persists across messages
2. Multiple threads with independent conversations
3. File upload + file_search query returns relevant content
4. Auth: logout redirects, no cross-user thread access, 401 without token
5. LangSmith: all traces visible with correct metadata

**After validation:** Update `PROGRESS.md` with Module 1 completion status

---

## File Tree Summary

```
backend/
  main.py, config.py, auth.py, database.py
  requirements.txt, .env.example
  services/__init__.py, openai_service.py
  routers/__init__.py, threads.py, chat.py, files.py
  models/__init__.py, threads.py, chat.py

frontend/
  package.json, vite.config.ts, tsconfig.json, index.html
  .env.example
  src/
    main.tsx, App.tsx, index.css
    lib/supabase.ts, api.ts
    contexts/AuthContext.tsx
    components/ProtectedRoute.tsx
    components/chat/ChatLayout.tsx, ThreadSidebar.tsx, ThreadItem.tsx
    components/chat/ChatArea.tsx, MessageList.tsx, MessageBubble.tsx
    components/chat/ChatInput.tsx, FileUploadButton.tsx
    hooks/useThreads.ts, useChat.ts
    pages/LoginPage.tsx, ChatPage.tsx
    types/index.ts
```

## Task Sequencing

```
1 Scaffolding -> 2 Supabase -> 3 Auth -> 4 Thread API -> 5 Chat UI -> 6 OpenAI Backend -> 7 SSE Frontend -> 8 File Upload -> 9 LangSmith -> 10 E2E
```

Tasks 1-4 are sequential foundations. Tasks 5-7 are the core integration (most likely to need iteration). Tasks 8-9 are additive features. Task 10 is validation.
